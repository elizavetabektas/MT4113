ylab("Sample Variance") +
ylim(0, max(samp_var))
source('~/.active-rstudio-document')
problemA <- function(NRepeat, # number of Monte Carlo simulations
X = x,
Y = y,
probability = prob){
N <- length(X)  # we know x and y have same length, so we only need one of them
all_prob <- rep(NA, NRepeat) # to store results
# Looping over number of Monte Carlo simulations
set.seed(13200)
for (i in seq(NRepeat)){
# bootstrapping data
bootData_x <- sample(X, size = N, replace = T) # bootstrap
bootData_y <- sample(Y, size = N, replace = T)
# Compute probability
P_xy <- sum(bootData_x > bootData_y)/N
all_prob[i] <- P_xy
}
all_prob <- c(all_prob, probability)
sample_variance <- var(all_prob) # calculate sample variance
return(sample_variance)
}
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK", setup_strategy = "sequential")
registerDoParallel(cl)
clusterExport(cl = cl, varlist = c("x", "y", "prob"))    # passing arguments to clusters
n_monte_carlo <- seq(1, 1000, 10)        # using big steps to save computing power
# looping over all numbers of Monte Carlo simulations using parallelised sapply
samp_var <- parSapply(X=n_monte_carlo, FUN=problemA, cl=cl)
# stopping cluster
stopCluster(cl)
ggplot(data.frame(x = n_monte_carlo, y = samp_var), mapping=aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
xlab("Number of Monte Carlo Simulations") +
ylab("Sample Variance") +
ylim(0, max(samp_var))
problemA <- function(NRepeat, # number of Monte Carlo simulations
X = x,
Y = y,
probability = prob){
N <- length(X)  # we know x and y have same length, so we only need one of them
all_prob <- rep(NA, NRepeat) # to store results
# Looping over number of Monte Carlo simulations
#set.seed(13200)
for (i in seq(NRepeat)){
# bootstrapping data
bootData_x <- sample(X, size = N, replace = T) # bootstrap
bootData_y <- sample(Y, size = N, replace = T)
# Compute probability
P_xy <- sum(bootData_x > bootData_y)/N
all_prob[i] <- P_xy
}
all_prob <- c(all_prob, probability)
sample_variance <- var(all_prob) # calculate sample variance
return(sample_variance)
}
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK", setup_strategy = "sequential")
registerDoParallel(cl)
clusterExport(cl = cl, varlist = c("x", "y", "prob"))    # passing arguments to clusters
n_monte_carlo <- seq(1, 10000, 1000)        # using big steps to save computing power
# looping over all numbers of Monte Carlo simulations using parallelised sapply
samp_var <- parSapply(X=n_monte_carlo, FUN=problemA, cl=cl)
# stopping cluster
stopCluster(cl)
ggplot(data.frame(x = n_monte_carlo, y = samp_var), mapping=aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
xlab("Number of Monte Carlo Simulations") +
ylab("Sample Variance") +
ylim(0, max(samp_var))
problemA <- function(NRepeat, # number of Monte Carlo simulations
X = x,
Y = y,
probability = prob){
N <- length(X)  # we know x and y have same length, so we only need one of them
all_prob <- rep(NA, NRepeat) # to store results
# Looping over number of Monte Carlo simulations
#set.seed(13200)
for (i in seq(NRepeat)){
# bootstrapping data
bootData_x <- sample(X, size = N, replace = T) # bootstrap
bootData_y <- sample(Y, size = N, replace = T)
# Compute probability
P_xy <- sum(bootData_x > bootData_y)/N
all_prob[i] <- P_xy
}
all_prob <- c(all_prob, probability)
sample_variance <- var(all_prob) # calculate sample variance
return(sample_variance)
}
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK", setup_strategy = "sequential")
registerDoParallel(cl)
clusterExport(cl = cl, varlist = c("x", "y", "prob"))    # passing arguments to clusters
n_monte_carlo <- seq(1, 10000, 100)        # using big steps to save computing power
# looping over all numbers of Monte Carlo simulations using parallelised sapply
samp_var <- parSapply(X=n_monte_carlo, FUN=problemA, cl=cl)
# stopping cluster
stopCluster(cl)
ggplot(data.frame(x = n_monte_carlo, y = samp_var), mapping=aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
xlab("Number of Monte Carlo Simulations") +
ylab("Sample Variance") +
ylim(0, max(samp_var))
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(foreach)
library(tidyverse)
library(doParallel)
set.seed(13200)                 # for reproducibility of the whole document
N <- 10000                       # number of samples from each distribution
x <- rnorm(N,4,sqrt(10))       # sampling from N and U distributions
y <- runif(N,2,8)
prob <- sum(x > y)/N          # calculating P(X>Y)
NRepeat <- 4999 # number of bootstrapped samples (total 5000 with observed test
# statistic so we can do shapiro test)
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK",setup_strategy = "sequential")
registerDoParallel(cl)
clusterExport(cl = cl, varlist = c("x", "y", "N"))    # give clusters variables
# Run code in parallel
all_prob <- foreach(i = 1:NRepeat, .combine = "c") %dopar% {
# Resample with replacement
bootData_x <- sample(x, N, T) # bootstrap
bootData_y <- sample(y, N, T)
# Compute probability X>Y for this sample
P_xy <- sum(bootData_x > bootData_y)/N
P_xy
}
stopCluster(cl)  # close the cluster
all_prob <- c(all_prob, prob)   # add observed prob to other probs
# plot sampling distribution and observed probability
ggplot(data.frame(x=all_prob), aes(x=x)) +
geom_histogram(aes(y=..density..), color = "darkcyan", fill="darkcyan") +
geom_vline(xintercept=prob, color = "darkblue", size=1) +
xlab("Test Statistic") +
ylab("Density")
# Mean and variance of test statistics
samp_dist_mean <- mean(all_prob)
samp_dist_sd <- sd(all_prob)
# plotting histogram and pdf (assuming distribution is normal) using all_prob
ggplot(data.frame(x=all_prob), aes(x)) +
geom_histogram(mapping=aes(y=..density..),
color = "darkcyan", fill="darkcyan") +
geom_function(fun=dnorm,
args = list(mean = samp_dist_mean, sd = samp_dist_sd),
color="darkblue", size=1) +
xlab("Test Statistic") +
ylab("Probability Density")
problemA <- function(NRepeat, # number of Monte Carlo simulations
X = x,
Y = y){
N <- length(X)  # we know x and y have same length, so we only need one of them
all_prob <- rep(NA, NRepeat) # to store results
# Looping over number of Monte Carlo simulations
set.seed(13200)
for (i in seq(NRepeat)){
# bootstrapping data
bootData_x <- sample(X, size = N, replace = T) # bootstrap
bootData_y <- sample(Y, size = N, replace = T)
# Compute probability
P_xy <- sum(bootData_x > bootData_y)/N
all_prob[i] <- P_xy
}
sample_variance <- var(all_prob) # calculate sample variance
return(sample_variance)
}
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK", setup_strategy = "sequential")
registerDoParallel(cl)
clusterExport(cl = cl, varlist = c("x", "y", "prob"))    # passing arguments to clusters
n_monte_carlo <- seq(1, 1000, 10)        # using big steps to save computing power
# looping over all numbers of Monte Carlo simulations using parallelised sapply
samp_var <- parSapply(X=n_monte_carlo, FUN=problemA, cl=cl)
# stopping cluster
stopCluster(cl)
ggplot(data.frame(x = n_monte_carlo, y = samp_var), mapping=aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
xlab("Number of Monte Carlo Simulations") +
ylab("Sample Variance") +
ylim(0, max(samp_var))
problemB <- function(p, NRepeat = 10000){
tournaments <- rep(NA, NRepeat) # store number of tournaments and win_rate for latter tasks
win_rate <- c()
# loop over the number of Monte Carlo simulations
for (i in 1:NRepeat){
games <- rbinom(n = 3, size = 1, prob = p) # have to play at least 3 games
wins <- length(games[games==1])        # how many are wins/losses
losses <- length(games[games==0])
# while there are <7 wins or <3 losses, keep playing
while ((wins < 7) & (losses < 3)){
games <- c(games, rbinom(n = 1, size = 1, prob = p))
wins <- length(games[games==1])
losses <- length(games[games==0])
}
tournaments[i] <- length(games)   # store no. tournaments played per simulation
win_rate[i] <- wins/length(games)    # and win_rate per simulation
}
mean_tournaments <- mean(tournaments)  # find mean no. tournaments & win rate
mean_win_rate <- mean(win_rate)
return(c(mean_tournaments, mean_win_rate))
}
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK", setup_strategy = "sequential")
registerDoParallel(cl)
p <- seq(0,1,0.05)              # for each probability, run problemB
res <- parSapply(X=p, FUN=problemB, cl=cl)    # parallelised sapply
stopCluster(cl)
tournaments <- res[1,]       # extract tournaments for each probability
win_rate <- res[2,]         # same for win_rate (for next task)
# plot probability against total matches played
ggplot(data.frame(x=p, y=tournaments), aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
ylim(0, max(tournaments)) +
xlab("Probability of Winning") + ylab("Total Matches Played")
ggplot(data.frame(x=p, y=win_rate), aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
xlab("Assumed Win Rate") +
ylab("Observed Win Rate") +
geom_line(mapping=aes(x=y), linetype="dotted")
shapiro.test(all_prob)$p.value
shapiro.test(all_prob)$p.value
shapiro.test(all_prob)$p.value
shapiro.test(all_prob)$p.value
#loading libraries
library(foreach)
library(tidyverse)
library(doParallel)
set.seed(13200)                 # for reproducibility of the whole document
N <- 10000                       # number of samples from each distribution
x <- rnorm(N,4,sqrt(10))       # sampling from N and U distributions
y <- runif(N,2,8)
prob <- sum(x > y)/N          # calculating P(X>Y)
NRepeat <- 4999 # number of bootstrapped samples (total 5000 with observed test
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK",setup_strategy = "sequential")
registerDoParallel(cl)
clusterExport(cl = cl, varlist = c("x", "y", "N"))    # give clusters variables
# Run code in parallel
all_prob <- foreach(i = 1:NRepeat, .combine = "c") %dopar% {
# Resample with replacement
bootData_x <- sample(x, N, T) # bootstrap
bootData_y <- sample(y, N, T)
# Compute probability X>Y for this sample
P_xy <- sum(bootData_x > bootData_y)/N
P_xy
}
stopCluster(cl)  # close the cluster
all_prob <- c(all_prob, prob)   # add observed prob to other probs
normality_test <- shapiro.test(all_prob)$p.value
# plot sampling distribution and observed probability
ggplot(data.frame(x=all_prob), aes(x=x)) +
geom_histogram(aes(y=..density..), color = "darkcyan", fill="darkcyan") +
geom_vline(xintercept=prob, color = "darkblue", size=1) +
xlab("Test Statistic") +
ylab("Density")
# Mean and variance of test statistics
samp_dist_mean <- mean(all_prob)
samp_dist_sd <- sd(all_prob)
# plotting histogram and pdf (assuming distribution is normal) using all_prob
ggplot(data.frame(x=all_prob), aes(x)) +
geom_histogram(mapping=aes(y=..density..),
color = "darkcyan", fill="darkcyan") +
geom_function(fun=dnorm,
args = list(mean = samp_dist_mean, sd = samp_dist_sd),
color="darkblue", size=1) +
xlab("Test Statistic") +
ylab("Probability Density")
problemA <- function(NRepeat, # number of Monte Carlo simulations
X = x,
Y = y){
N <- length(X)  # we know x and y have same length, so we only need one of them
all_prob <- rep(NA, NRepeat) # to store results
# Looping over number of Monte Carlo simulations
set.seed(13200)
for (i in seq(NRepeat)){
# bootstrapping data
bootData_x <- sample(X, size = N, replace = T) # bootstrap
bootData_y <- sample(Y, size = N, replace = T)
# Compute probability
P_xy <- sum(bootData_x > bootData_y)/N
all_prob[i] <- P_xy
}
sample_variance <- var(all_prob) # calculate sample variance
return(sample_variance)
}
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK", setup_strategy = "sequential")
registerDoParallel(cl)
clusterExport(cl = cl, varlist = c("x", "y", "prob"))    # passing arguments to clusters
n_monte_carlo <- seq(1, 1000, 10)        # using big steps to save computing power
# looping over all numbers of Monte Carlo simulations using parallelised sapply
samp_var <- parSapply(X=n_monte_carlo, FUN=problemA, cl=cl)
# stopping cluster
stopCluster(cl)
ggplot(data.frame(x = n_monte_carlo, y = samp_var), mapping=aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
xlab("Number of Monte Carlo Simulations") +
ylab("Sample Variance") +
ylim(0, max(samp_var))
#loading libraries
library(foreach)
library(tidyverse)
library(doParallel)
set.seed(13200)                 # for reproducibility of the whole document
N <- 10000                       # number of samples from each distribution
x <- rnorm(N,4,sqrt(10))       # sampling from N and U distributions
y <- runif(N,2,8)
prob <- sum(x > y)/N          # calculating P(X>Y)
NRepeat <- 4999 # number of bootstrapped samples (total 5000 with observed test
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK",setup_strategy = "sequential")
registerDoParallel(cl)
clusterExport(cl = cl, varlist = c("x", "y", "N"))    # give clusters variables
# Run code in parallel
all_prob <- foreach(i = 1:NRepeat, .combine = "c") %dopar% {
# Resample with replacement
bootData_x <- sample(x, N, T) # bootstrap
bootData_y <- sample(y, N, T)
# Compute probability X>Y for this sample
P_xy <- sum(bootData_x > bootData_y)/N
P_xy
}
stopCluster(cl)  # close the cluster
all_prob <- c(all_prob, prob)   # add observed prob to other probs
normality_test <- shapiro.test(all_prob)$p.value
# plot sampling distribution and observed probability
ggplot(data.frame(x=all_prob), aes(x=x)) +
geom_histogram(aes(y=..density..), color = "darkcyan", fill="darkcyan") +
geom_vline(xintercept=prob, color = "darkblue", size=1) +
xlab("Test Statistic") +
ylab("Density")
# Mean and variance of test statistics
samp_dist_mean <- mean(all_prob)
samp_dist_sd <- sd(all_prob)
# plotting histogram and pdf (assuming distribution is normal) using all_prob
ggplot(data.frame(x=all_prob), aes(x)) +
geom_histogram(mapping=aes(y=..density..),
color = "darkcyan", fill="darkcyan") +
geom_function(fun=dnorm,
args = list(mean = samp_dist_mean, sd = samp_dist_sd),
color="darkblue", size=1) +
xlab("Test Statistic") +
ylab("Probability Density")
problemA <- function(NRepeat, # number of Monte Carlo simulations
X = x,
Y = y){
N <- length(X)  # we know x and y have same length, so we only need one of them
all_prob <- rep(NA, NRepeat) # to store results
# Looping over number of Monte Carlo simulations
set.seed(13200)
for (i in seq(NRepeat)){
# bootstrapping data
bootData_x <- sample(X, size = N, replace = T) # bootstrap
bootData_y <- sample(Y, size = N, replace = T)
# Compute probability
P_xy <- sum(bootData_x > bootData_y)/N
all_prob[i] <- P_xy
}
sample_variance <- var(all_prob) # calculate sample variance
return(sample_variance)
}
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK", setup_strategy = "sequential")
registerDoParallel(cl)
clusterExport(cl = cl, varlist = c("x", "y", "prob"))    # passing arguments to clusters
n_monte_carlo <- seq(1, 1000, 10)        # using big steps to save computing power
# looping over all numbers of Monte Carlo simulations using parallelised sapply
samp_var <- parSapply(X=n_monte_carlo, FUN=problemA, cl=cl)
# stopping cluster
stopCluster(cl)
ggplot(data.frame(x = n_monte_carlo, y = samp_var), mapping=aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
xlab("Number of Monte Carlo Simulations") +
ylab("Sample Variance") +
ylim(0, max(samp_var))
problemB <- function(p, NRepeat = 10000){
tournaments <- rep(NA, NRepeat) # store number of tournaments and win_rate for latter tasks
win_rate <- c()
# loop over the number of Monte Carlo simulations
for (i in 1:NRepeat){
games <- rbinom(n = 3, size = 1, prob = p) # have to play at least 3 games
wins <- length(games[games==1])        # how many are wins/losses
losses <- length(games[games==0])
# while there are <7 wins or <3 losses, keep playing
while ((wins < 7) & (losses < 3)){
games <- c(games, rbinom(n = 1, size = 1, prob = p))
wins <- length(games[games==1])
losses <- length(games[games==0])
}
tournaments[i] <- length(games)   # store no. tournaments played per simulation
win_rate[i] <- wins/length(games)    # and win_rate per simulation
}
mean_tournaments <- mean(tournaments)  # find mean no. tournaments & win rate
mean_win_rate <- mean(win_rate)
return(c(mean_tournaments, mean_win_rate))
}
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK", setup_strategy = "sequential")
registerDoParallel(cl)
p <- seq(0,1,0.05)              # for each probability, run problemB
res <- parSapply(X=p, FUN=problemB, cl=cl)    # parallelised sapply
stopCluster(cl)
tournaments <- res[1,]       # extract tournaments for each probability
win_rate <- res[2,]         # same for win_rate (for next task)
# plot probability against total matches played
ggplot(data.frame(x=p, y=tournaments), aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
ylim(0, max(tournaments)) +
xlab("Probability of Winning") + ylab("Total Matches Played")
ggplot(data.frame(x=p, y=win_rate), aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
xlab("Assumed Win Rate") +
ylab("Observed Win Rate") +
geom_line(mapping=aes(x=y), linetype="dotted")
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK", setup_strategy = "sequential")
registerDoParallel(cl)
clusterExport(cl = cl, varlist = c("x", "y", "prob"))    # passing arguments to clusters
n_monte_carlo <- seq(10, 1000, 10)        # using big steps to save computing power
# looping over all numbers of Monte Carlo simulations using parallelised sapply
samp_var <- parSapply(X=n_monte_carlo, FUN=problemA, cl=cl)
samp_var
R.Version()
setwd("~/Desktop/MT4113/Assignment3")
install.packages(dslabs)
install.packages('dslabs')
library(dslabs)
data(divorce_margarine)
library(dslabs)
data(divorce_margarine)
force(divorce_margarine)
data(heights)
force(heights)
max(heights$height)
min(heights$height)
data(murders)
force(murders)
data("research_funding_rates")
force(research_funding_rates)
View(murders)
View(heights)
View(heights)
data("reported_heights")
force(reported_heights)
View(reported_heights)
data(gapminder)
force(gapminder)
View(gapminder)
levels(murders$region)
library(dplyr)
group_by(murders, region) %>%
summarise(
count = n(),
rate = total/population)
murders %>% group_by(region) %>%
summarise(
count = n(),
rate = total/population)
murders %>% group_by(region) %>%
summarise(
#count = n(),
rate = total/population)
murders$rate <- murders$total / murders$population
murders %>% group_by(region) %>%
summarise(
mean = mean(rate)
)
# Compute the analysis of variance
res.aov <- aov(rate ~ region, data = murders)
res.aov
# Summary of the analysis
summary(res.aov)
kruskal.test(rate ~ region, data = murders)
hist(murders$rate)
hist(murders$total)
hist(murders$population)
hist(murders$rate)
dispersion_test <- function(x)
{
res <- 1-2 * abs((1 - pchisq((sum((x - mean(x))^2)/mean(x)), length(x) - 1))-0.5)
cat("Dispersion test of count data:\n",
length(x), " data points.\n",
"Mean: ",mean(x),"\n",
"Variance: ",var(x),"\n",
"Probability of being drawn from Poisson distribution: ",
round(res, 3),"\n", sep = "")
invisible(res)
}
dispersion_test(murders$rate)
library(vcd)
library(MASS)
# estimate the parameters
fit1 <- fitdistr(murders$rate, "exponential")
# goodness of fit test
ks.test(ex, "pexp", fit1$estimate) # p-value > 0.05 -> distribution not refused
# goodness of fit test
ks.test(murders$rate, "pexp", fit1$estimate) # p-value > 0.05 -> distribution not refused
# estimate the parameters
fit1 <- fitdistr(murders$rate, "negative binomial")
# goodness of fit test
ks.test(murders$rate, "pnbinom", fit1$estimate) # p-value > 0.05 -> distribution not refused
murders$rate
# estimate the parameters
fit1 <- fitdistr(murders$rate, "negative binomial")
warnings()
# estimate the parameters
fit1 <- fitdistr(murders$rate, "gamma")
?pgamma
# goodness of fit test
ks.test(murders$rate, "pgamma", fit1$estimate) # p-value > 0.05 -> distribution not refused
fit1
