geom_histogram(aes(y=..density..), color = "darkcyan", fill="darkcyan") +
geom_vline(xintercept=prob, color = "darkblue", size=1) +
xlab("Test Statistic") +
ylab("Density")
# Mean and variance of test statistics
samp_dist_mean <- mean(all_prob)
samp_dist_sd <- sd(all_prob)
# plotting histogram and pdf (assuming distribution is normal) using all_prob
ggplot(data.frame(x=all_prob), aes(x)) +
geom_histogram(mapping=aes(y=..density..),
color = "darkcyan", fill="darkcyan") +
geom_function(fun=dnorm,
args = list(mean = samp_dist_mean, sd = samp_dist_sd),
color="darkblue", size=1) +
xlab("Test Statistic") +
ylab("Probability Density")
problemA <- function(NRepeat, # number of Monte Carlo simulations
X = x,
Y = y){
N <- length(X)  # we know x and y have same length, so we only need one of them
all_prob <- rep(NA, NRepeat) # to store results
# Looping over number of Monte Carlo simulations
set.seed(13200)
for (i in seq(NRepeat)){
# bootstrapping data
bootData_x <- sample(X, size = N, replace = T) # bootstrap
bootData_y <- sample(Y, size = N, replace = T)
# Compute probability
P_xy <- sum(bootData_x > bootData_y)/N
all_prob[i] <- P_xy
}
sample_variance <- var(all_prob) # calculate sample variance
return(sample_variance)
}
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK", setup_strategy = "sequential")
registerDoParallel(cl)
clusterExport(cl = cl, varlist = c("x", "y", "prob"))    # passing arguments to clusters
n_monte_carlo <- seq(1, 1000, 10)        # using big steps to save computing power
# looping over all numbers of Monte Carlo simulations using parallelised sapply
samp_var <- parSapply(X=n_monte_carlo, FUN=problemA, cl=cl)
# stopping cluster
stopCluster(cl)
ggplot(data.frame(x = n_monte_carlo, y = samp_var), mapping=aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
xlab("Number of Monte Carlo Simulations") +
ylab("Sample Variance") +
ylim(0, max(samp_var))
problemB <- function(p, NRepeat = 10000){
tournaments <- rep(NA, NRepeat) # store number of tournaments and win_rate for latter tasks
win_rate <- c()
# loop over the number of Monte Carlo simulations
for (i in 1:NRepeat){
games <- rbinom(n = 3, size = 1, prob = p) # have to play at least 3 games
wins <- length(games[games==1])        # how many are wins/losses
losses <- length(games[games==0])
# while there are <7 wins or <3 losses, keep playing
while ((wins < 7) & (losses < 3)){
games <- c(games, rbinom(n = 1, size = 1, prob = p))
wins <- length(games[games==1])
losses <- length(games[games==0])
}
tournaments[i] <- length(games)   # store no. tournaments played per simulation
win_rate[i] <- wins/length(games)    # and win_rate per simulation
}
mean_tournaments <- mean(tournaments)  # find mean no. tournaments & win rate
mean_win_rate <- mean(win_rate)
return(c(mean_tournaments, mean_win_rate))
}
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK", setup_strategy = "sequential")
registerDoParallel(cl)
p <- seq(0,1,0.05)              # for each probability, run problemB
res <- parSapply(X=p, FUN=problemB, cl=cl)    # parallelised sapply
stopCluster(cl)
tournaments <- res[1,]       # extract tournaments for each probability
win_rate <- res[2,]         # same for win_rate (for next task)
# plot probability against total matches played
ggplot(data.frame(x=p, y=tournaments), aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
ylim(0, max(tournaments)) +
xlab("Probability of Winning") + ylab("Total Matches Played")
ggplot(data.frame(x=p, y=win_rate), aes(x=x, y=y)) +
geom_line(color="darkblue", size=1) +
xlab("Assumed Win Rate") +
ylab("Observed Win Rate") +
geom_line(mapping=aes(x=y), linetype="dotted")
# Register a parallel backend
cl <- makeCluster(spec = 4, type = "PSOCK", setup_strategy = "sequential")
registerDoParallel(cl)
clusterExport(cl = cl, varlist = c("x", "y", "prob"))    # passing arguments to clusters
n_monte_carlo <- seq(10, 1000, 10)        # using big steps to save computing power
# looping over all numbers of Monte Carlo simulations using parallelised sapply
samp_var <- parSapply(X=n_monte_carlo, FUN=problemA, cl=cl)
samp_var
R.Version()
setwd("~/Desktop/MT4113/Practical7")
mining_data <- read.csv("./MiningData.csv")
max(mining_data$angle)
max(mining_data$width / mining_data$depth)
which.max(mining_data$width / mining_data$depth)
mining_data
mining_data$angle[which.max(mining_data$width / mining_data$depth)]
mining_data$width[which.max(mining_data$width / mining_data$depth)]
mining_data$depth[which.max(mining_data$width / mining_data$depth)]
# As x tends to infinity, y tends to alpha
# take the maximum value of x, initialise alpha to be the angle
alpha_0 <- mining_data$angle[which.max(mining_data$width / mining_data$depth)]
initial_x <- mining_data$width[which.max(mining_data$width / mining_data$depth)] /
mining_data$depth[which.max(mining_data$width / mining_data$depth)]
initial_y <- alpha_0
beta_0 <- 1 #randomly chosen, no real reason
model <- function(x, alpha, beta){
y <- alpha * (1 - exp(-beta*x))
}
y <- mining_data$angle
x <- sort(mining_data$width / mining_data$depth)
y_curve <- sapply(x, model, alpha = alpha_0, beta = beta_0)
plot(x, y)
lines(x,y_curve)
theta <- c(alpha_0, beta_0)
RSS <- function(theta, data){
alpha <- theta[0]
beta <- theta[1]
y <- data$angle
x <- sort(data$width / data$depth)
s1 <- 1 - exp(-as.vector(beta)*x)
s2 <- alpha*s1
R_theta <- sum((y-s2)^2)
return(R_theta)
}
RSS <- function(theta, data){
alpha <- theta[1]
beta <- theta[2]
y <- data$angle
x <- sort(data$width / data$depth)
s1 <- 1 - exp(-as.vector(beta)*x)
s2 <- alpha*s1
R_theta <- sum((y-s2)^2)
return(R_theta)
}
# Exercise 3
library(numDeriv)
?grad
mining_data <- read.csv("./MiningData.csv")
# As x tends to infinity, y tends to alpha
# take the maximum value of x, initialise alpha to be the angle
alpha_0 <- mining_data$angle[which.max(mining_data$width / mining_data$depth)]
initial_x <- mining_data$width[which.max(mining_data$width / mining_data$depth)] /
mining_data$depth[which.max(mining_data$width / mining_data$depth)]
initial_y <- alpha_0
beta_0 <- 1 #randomly chosen, no real reason
theta_0 <- c(alpha_0, beta_0)
model <- function(x, alpha, beta){
y <- alpha * (1 - exp(-beta*x))
}
y <- mining_data$angle
x <- sort(mining_data$width / mining_data$depth)
y_curve <- sapply(x, model, alpha = alpha_0, beta = beta_0)
plot(x, y)
lines(x,y_curve)
RSS <- function(theta, data){
alpha <- theta[1]
beta <- theta[2]
y <- data$angle
x <- sort(data$width / data$depth)
s1 <- 1 - exp(-as.vector(beta)*x)
s2 <- alpha*s1
R_theta <- sum((y-s2)^2)
return(R_theta)
}
# Exercise 3
library(numDeriv)
gradient <- grad(RSS, theta_0, data = mining_data)
?hessian
second_derivative <- hessian(RSS, theta_0, data = mining_data)
norm(c(3,4))
norm(c(3,4), type='2')
Newton <- function(start, f, data, epsilon, maxit){
theta <- start
convergence <- 2*epsilon
counter <- 0
while (convergence >= epsilon){
gradient <- grad(f, theta, data = data)
second_derivative <- hessian(f, theta, data = data)
delta <- solve(second_derivative, - gradient)
theta <- theta + delta
convergence <- norm(delta, type='2') / norm(theta, type='2')
counter <- counter + 1
if (counter > maxit){
break
}
}
return(c(theta, counter))
}
Newton_output <- Newton(theta_0, RSS, mining_data, 1e-5, 1000)
beta_0 <- 34 #randomly chosen, no real reason
theta_0 <- c(alpha_0, beta_0)
Newton <- function(start, f, data, epsilon, maxit){
theta <- start
convergence <- 2*epsilon
counter <- 0
while (convergence >= epsilon){
gradient <- grad(f, theta, data = data)
second_derivative <- hessian(f, theta, data = data)
delta <- solve(second_derivative, - gradient)
theta <- theta + delta
convergence <- norm(delta, type='2') / norm(theta, type='2')
counter <- counter + 1
if (counter > maxit){
break
}
}
return(c(theta, counter))
}
Newton_output <- Newton(theta_0, RSS, mining_data, 1e-5, 1000)
mining_data <- read.csv("./MiningData.csv")
# As x tends to infinity, y tends to alpha
# take the maximum value of x, initialise alpha to be the angle
alpha_0 <- mining_data$angle[which.max(mining_data$width / mining_data$depth)]
initial_x <- mining_data$width[which.max(mining_data$width / mining_data$depth)] /
mining_data$depth[which.max(mining_data$width / mining_data$depth)]
initial_y <- alpha_0
beta_0 <- 34 #randomly chosen, no real reason
theta_0 <- c(alpha_0, beta_0)
model <- function(x, alpha, beta){
y <- alpha * (1 - exp(-beta*x))
}
y <- mining_data$angle
x <- sort(mining_data$width / mining_data$depth)
y_curve <- sapply(x, model, alpha = alpha_0, beta = beta_0)
plot(x, y)
lines(x,y_curve)
RSS <- function(theta, data){
alpha <- theta[1]
beta <- theta[2]
y <- data$angle
x <- sort(data$width / data$depth)
s1 <- 1 - exp(-as.vector(beta)*x)
s2 <- alpha*s1
R_theta <- sum((y-s2)^2)
return(R_theta)
}
# Exercise 3
library(numDeriv)
gradient <- grad(RSS, theta_0, data = mining_data)
second_derivative <- hessian(RSS, theta_0, data = mining_data)
Newton <- function(start, f, data, epsilon, maxit){
theta <- start
convergence <- 2*epsilon
counter <- 0
while (convergence >= epsilon){
gradient <- grad(f, theta, data = data)
second_derivative <- hessian(f, theta, data = data)
delta <- solve(second_derivative, - gradient)
theta <- theta + delta
convergence <- norm(delta, type='2') / norm(theta, type='2')
counter <- counter + 1
if (counter > maxit){
break
}
}
return(c(theta, counter))
}
Newton_output <- Newton(theta_0, RSS, mining_data, 1e-5, 1000)
Newton_alpha <- Newton_output[1]
Newton_beta <- Newton_output[2]
Newton_iterations <- Newton_output[3]
nlm(RSS, theta_0)
nlm(RSS, theta_0, data=mining_data)
?nlm
nlm_result <- nlm(RSS, theta_0, data=mining_data)
nlm_alpha <- nlm_result$estimate[1]
nlm_beta <- nlm_result$estimate[2]
Newton2 <- function(start, f, data, epsilon, maxit, gfn, Hfn){
theta <- start
convergence <- 2*epsilon
counter <- 0
while (convergence >= epsilon){
gradient <- gfn(theta, data)
second_derivative <- Hfn(theta, data)
delta <- solve(second_derivative, - gradient)
theta <- theta + delta
convergence <- norm(delta, type='2') / norm(theta, type='2')
counter <- counter + 1
if (counter > maxit){
break
}
}
return(c(theta, counter))
}
Newton2_output <- Newton2(theta_0, RSS, mining_data, 1e-5, 1000, df, d2f)
# Compute gradient vector for objective function f
# INPUTS: theta: parameter vector data: dataframe with
# x and y columns OUTPUTS: vector of gradients in each
# direction
df <- function(theta, data) {
# compute residuals r for each data point
pred <- theta[1] * (1 - exp(-theta[2] * data$x))
r <- data$y - pred
# df/da
dfa <- -2 * (1 - exp(-theta[2] * data$x)) * r
# df/db
dfb <- -2 * theta[1] * data$x * exp(-theta[2] * data$x) *
r
return(c(sum(dfa), sum(dfb)))
}
# Compute Hessian matrix for objective function f
# INPUTS: theta: parameter vector data: dataframe with
# x and y columns OUTPUTS: Hessian matrix of second
# derivatives
d2f <- function(theta, data) {
# compute residuals for each data point
pred <- theta[1] * (1 - exp(-theta[2] * data$x))
r <- data$y - pred
# compute d2f / da2
dfaa <- 2 * sum((1 - exp(-theta[2] * data$x))^2)
# d2f / db2
dfbb <- 2 * sum(theta[1]^2 * data$x^2 * exp(-2 * theta[2] *
data$x) + r * theta[1] * data$x^2 * exp(-theta[2] *
data$x))
# d2f / dab
dfab <- 2 * sum(theta[1] * data$x * exp(-theta[2] *
data$x) * (1 - exp(-theta[2] * data$x)) - r * data$x *
exp(-theta[2] * data$x))
# H = hessian
H <- matrix(c(dfaa, dfab, dfab, dfbb), nrow = 2, ncol = 2)
return(H)
}
Newton2 <- function(start, f, data, epsilon, maxit, gfn, Hfn){
theta <- start
convergence <- 2*epsilon
counter <- 0
while (convergence >= epsilon){
gradient <- gfn(theta, data)
second_derivative <- Hfn(theta, data)
delta <- solve(second_derivative, - gradient)
theta <- theta + delta
convergence <- norm(delta, type='2') / norm(theta, type='2')
counter <- counter + 1
if (counter > maxit){
break
}
}
return(c(theta, counter))
}
Newton2_output <- Newton2(theta_0, RSS, mining_data, 1e-5, 1000, df, d2f)
Newton2 <- function(start, f, data, epsilon, maxit, gfn, Hfn){
theta <- start
convergence <- 2*epsilon
counter <- 0
while (convergence >= epsilon){
gradient <- gfn(theta, data)
second_derivative <- Hfn(theta, data)
print(gradient)
print(second_derivative)
delta <- solve(second_derivative, - gradient)
theta <- theta + delta
convergence <- norm(delta, type='2') / norm(theta, type='2')
counter <- counter + 1
if (counter > maxit){
break
}
}
return(c(theta, counter))
}
Newton2_output <- Newton2(theta_0, RSS, mining_data, 1e-5, 1000, df, d2f)
df(theta_0, mining_data)
# Compute gradient vector for objective function f
# INPUTS: theta: parameter vector data: dataframe with
# x and y columns OUTPUTS: vector of gradients in each
# direction
df <- function(theta, data) {
# compute residuals r for each data point
data$x <- data$width / data$depth
pred <- theta[1] * (1 - exp(-theta[2] * data$x))
r <- data$angle - pred
# df/da
dfa <- -2 * (1 - exp(-theta[2] * data$x)) * r
# df/db
dfb <- -2 * theta[1] * data$x * exp(-theta[2] * data$x) * r
return(c(sum(dfa), sum(dfb)))
}
df(theta_0, mining_data)
# Compute Hessian matrix for objective function f
# INPUTS: theta: parameter vector data: dataframe with
# x and y columns OUTPUTS: Hessian matrix of second
# derivatives
d2f <- function(theta, data) {
data$x <- data$width / data$depth
# compute residuals for each data point
pred <- theta[1] * (1 - exp(-theta[2] * data$x))
r <- data$y - pred
# compute d2f / da2
dfaa <- 2 * sum((1 - exp(-theta[2] * data$x))^2)
# d2f / db2
dfbb <- 2 * sum(theta[1]^2 * data$x^2 * exp(-2 * theta[2] *
data$x) + r * theta[1] * data$x^2 * exp(-theta[2] *
data$x))
# d2f / dab
dfab <- 2 * sum(theta[1] * data$x * exp(-theta[2] *
data$x) * (1 - exp(-theta[2] * data$x)) - r * data$x *
exp(-theta[2] * data$x))
# H = hessian
H <- matrix(c(dfaa, dfab, dfab, dfbb), nrow = 2, ncol = 2)
return(H)
}
d2f(theta_0, mining_data)
# Compute Hessian matrix for objective function f
# INPUTS: theta: parameter vector data: dataframe with
# x and y columns OUTPUTS: Hessian matrix of second
# derivatives
d2f <- function(theta, data) {
data$x <- data$width / data$depth
# compute residuals for each data point
pred <- theta[1] * (1 - exp(-theta[2] * data$x))
r <- data$angle - pred
# compute d2f / da2
dfaa <- 2 * sum((1 - exp(-theta[2] * data$x))^2)
# d2f / db2
dfbb <- 2 * sum(theta[1]^2 * data$x^2 * exp(-2 * theta[2] *
data$x) + r * theta[1] * data$x^2 * exp(-theta[2] *
data$x))
# d2f / dab
dfab <- 2 * sum(theta[1] * data$x * exp(-theta[2] *
data$x) * (1 - exp(-theta[2] * data$x)) - r * data$x *
exp(-theta[2] * data$x))
# H = hessian
H <- matrix(c(dfaa, dfab, dfab, dfbb), nrow = 2, ncol = 2)
return(H)
}
d2f(theta_0, mining_data)
Newton2 <- function(start, f, data, epsilon, maxit, gfn, Hfn){
theta <- start
convergence <- 2*epsilon
counter <- 0
while (convergence >= epsilon){
gradient <- gfn(theta, data)
second_derivative <- Hfn(theta, data)
print(gradient)
print(second_derivative)
delta <- solve(second_derivative, - gradient)
theta <- theta + delta
convergence <- norm(delta, type='2') / norm(theta, type='2')
counter <- counter + 1
if (counter > maxit){
break
}
}
return(c(theta, counter))
}
Newton2_output <- Newton2(theta_0, RSS, mining_data, 1e-5, 1000, df, d2f)
# Compute Hessian matrix for objective function f
# INPUTS: theta: parameter vector data: dataframe with
# x and y columns OUTPUTS: Hessian matrix of second
# derivatives
d2f <- function(theta, data) {
data$x <- data$width / data$depth
# compute residuals for each data point
pred <- theta[1] * (1 - exp(-theta[2] * data$x))
r <- data$angle - pred
# compute d2f / da2
dfaa <- 2 * sum((1 - exp(-theta[2] * data$x))^2)
# d2f / db2
dfbb <- 2 * sum(theta[1]^2 * data$x^2 * exp(-2 * theta[2] * data$x) + r * theta[1] * data$x^2 * exp(-theta[2] *
data$x))
# d2f / dab
dfab <- 2 * sum(theta[1] * data$x * exp(-theta[2] * data$x) * (1 - exp(-theta[2] * data$x)) - r * data$x *exp(-theta[2] * data$x))
# H = hessian
H <- matrix(c(dfaa, dfab, dfab, dfbb), nrow = 2, ncol = 2)
return(H)
}
Newton2_output <- Newton2(theta_0, RSS, mining_data, 1e-5, 1000, df, d2f)
Newton2 <- function(start, f, data, epsilon, maxit, gfn, Hfn){
theta <- start
convergence <- 2*epsilon
counter <- 0
while (convergence >= epsilon){
gradient <- gfn(theta, data)
second_derivative <- Hfn(theta, data)
print(theta)
delta <- solve(second_derivative, - gradient)
theta <- theta + delta
convergence <- norm(delta, type='2') / norm(theta, type='2')
counter <- counter + 1
if (counter > maxit){
break
}
}
return(c(theta, counter))
}
Newton2_output <- Newton2(theta_0, RSS, mining_data, 1e-5, 1000, df, d2f)
Newton2 <- function(start, f, data, epsilon, maxit, gfn, Hfn){
theta <- start
convergence <- 2*epsilon
counter <- 0
while (convergence >= epsilon){
gradient <- gfn(theta, data)
second_derivative <- Hfn(theta, data)
print(theta)
delta <- solve(second_derivative, - gradient)
theta <- theta + delta
convergence <- norm(delta, type='2') / norm(theta, type='2')
counter <- counter + 1
print(counter)
if (counter > maxit){
break
}
}
return(c(theta, counter))
}
Newton2_output <- Newton2(theta_0, RSS, mining_data, 1e-5, 1000, df, d2f)
